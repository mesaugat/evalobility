{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2beb1f",
   "metadata": {},
   "source": [
    "## Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d736890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenTelemetry configuration OTEL_EXPORTER_OTLP_TRACES_PROTOCOL is not supported by Datadog.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Observability initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "# Initialize LLM Observability\n",
    "LLMObs.enable(\n",
    "    site=os.getenv(\"DD_SITE\", \"datadoghq.com\"),\n",
    "    api_key=os.getenv(\"DD_API_KEY\"),\n",
    "    app_key=os.getenv(\"DD_APPLICATION_KEY\"),\n",
    "    project_name=\"wwktm-kb-agent-new\",\n",
    "    ml_app=\"strands-agents\"\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM Observability initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67f43",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We'll load our KB agent test dataset containing questions about wwktm policies, out-of-scope questions, and adversarial queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3120350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from CSV (v2 with 25 difficult samples)\n",
    "dataset = LLMObs.create_dataset_from_csv(\n",
    "    csv_path=\"kb_agent_dataset_v2.csv\",\n",
    "    dataset_name=\"wwktm-kb-agent-eval-v2\",\n",
    "    description=\"Difficult dataset for evaluating wwktm KB Agent - includes policy, out-of-scope, and adversarial questions\",\n",
    "    input_data_columns=[\"question\", \"policy_source\", \"category\", \"difficulty\", \"should_refuse\"],\n",
    "    expected_output_columns=[\"expected_answer\"],\n",
    "    csv_delimiter=\",\"\n",
    ")\n",
    "\n",
    "# print(f\"ðŸ“Š Dataset loaded with {len(dataset)} samples\")\n",
    "# dataset.as_dataframe()\n",
    "\n",
    "# Pull the dataset if your dataset has not changed\n",
    "# dataset = LLMObs.pull_dataset(\n",
    "#     dataset_name=\"wwktm-kb-agent-eval-v2\",\n",
    "#     project_name=\"wwktm-kb-agent-new\",\n",
    "#     version=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad40bff",
   "metadata": {},
   "source": [
    "## Import Agent Configuration\n",
    "\n",
    "We'll use our existing agent configuration to create agents with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c456bbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent configuration imported\n"
     ]
    }
   ],
   "source": [
    "from agent_config import create_bedrock_model, create_agent, SYSTEM_PROMPT\n",
    "from strands_tools import retrieve, use_agent\n",
    "\n",
    "# Model IDs for comparison\n",
    "MODEL_CLAUDE_HAIKU_4_5 = \"us.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "MODEL_AMAZON_NOVA_PRO = \"us.amazon.nova-pro-v1:0\"\n",
    "MODEL_AMAZON_NOVA_MICRO = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "print(\"âœ… Agent configuration imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c1fb7",
   "metadata": {},
   "source": [
    "## Define Task Function\n",
    "\n",
    "The task function runs our KB agent against a question and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeae13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_agent_task(input_data, config):\n",
    "    \"\"\"\n",
    "    Task function that runs the KB agent with the configured model.\n",
    "\n",
    "    Args:\n",
    "        input_data: Contains 'question', 'policy_source', and 'category'\n",
    "        config: Contains 'model_id' and optional guardrail settings\n",
    "\n",
    "    Returns:\n",
    "        dict with 'response' and 'model' keys\n",
    "    \"\"\"\n",
    "    # Create model with specified configuration\n",
    "    model = create_bedrock_model(\n",
    "        model_id=config.get(\"model_id\", MODEL_CLAUDE_HAIKU_4_5),\n",
    "    )\n",
    "\n",
    "    # Create agent with the model (no console output for experiments)\n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        callback_handler=None  # Disable console output\n",
    "    )\n",
    "\n",
    "    # Run the agent with the question\n",
    "    question = input_data[\"question\"]\n",
    "    result = agent(question)\n",
    "\n",
    "    # Extract text response\n",
    "    response_text = str(result)\n",
    "\n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"model\": config.get(\"model_id\"),\n",
    "        \"category\": input_data.get(\"category\", \"unknown\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aad864",
   "metadata": {},
   "source": [
    "## Define Evaluators\n",
    "\n",
    "We'll create evaluators to assess the agent's responses:\n",
    "1. **Semantic Match**: Check if the response contains key elements from the expected answer\n",
    "2. **Response Quality**: Basic validation of response quality\n",
    "3. **Hallucination Detection**: Check for obvious hallucination patterns\n",
    "4. **Out-of-Scope Handling**: Check if agent correctly refuses non-policy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c564e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluators defined (including out-of-scope handler)\n"
     ]
    }
   ],
   "source": [
    "def contains_key_info(input_data, output_data, expected_output):\n",
    "    \"\"\"\n",
    "    Check if the response contains key information from the expected answer.\n",
    "    Uses simple keyword matching for efficiency.\n",
    "    \"\"\"\n",
    "    response = output_data.get(\"response\", \"\").lower()\n",
    "    expected = expected_output.get(\"expected_answer\", \"\").lower()\n",
    "\n",
    "    # Extract key terms (numbers, important words)\n",
    "    # Look for numbers and key policy terms\n",
    "    number_pattern = r'\\d+'\n",
    "    expected_numbers = set(re.findall(number_pattern, expected))\n",
    "    response_numbers = set(re.findall(number_pattern, response))\n",
    "\n",
    "    # Check if key numbers match\n",
    "    if expected_numbers:\n",
    "        number_match = len(expected_numbers.intersection(response_numbers)) / len(expected_numbers)\n",
    "    else:\n",
    "        number_match = 1.0\n",
    "\n",
    "    # Check for key terms\n",
    "    key_terms = [term for term in expected.split() if len(term) > 4]\n",
    "    if key_terms:\n",
    "        term_matches = sum(1 for term in key_terms if term in response)\n",
    "        term_match = term_matches / len(key_terms)\n",
    "    else:\n",
    "        term_match = 1.0\n",
    "\n",
    "    # Combined score - pass if average is above 0.5\n",
    "    score = (number_match + term_match) / 2\n",
    "    return score >= 0.5\n",
    "\n",
    "\n",
    "def response_not_empty(input_data, output_data, expected_output):\n",
    "    \"\"\"\n",
    "    Basic check that the agent provided a non-empty response.\n",
    "    \"\"\"\n",
    "    response = output_data.get(\"response\", \"\")\n",
    "    return len(response.strip()) > 20\n",
    "\n",
    "\n",
    "def no_hallucination_indicators(input_data, output_data, expected_output):\n",
    "    \"\"\"\n",
    "    Check for common hallucination indicators in the response.\n",
    "    Returns True if no obvious hallucination patterns are detected.\n",
    "    \"\"\"\n",
    "    response = output_data.get(\"response\", \"\").lower()\n",
    "\n",
    "    # Patterns that might indicate uncertainty or hallucination\n",
    "    uncertainty_patterns = [\n",
    "        \"i don't have information\",\n",
    "        \"i cannot find\",\n",
    "        \"not specified in the\",\n",
    "        \"i'm not sure\",\n",
    "        \"i don't know\"\n",
    "    ]\n",
    "\n",
    "    # If the agent admits uncertainty, that's actually good (not hallucinating)\n",
    "    # But we want to flag when it gives confident wrong answers\n",
    "    for pattern in uncertainty_patterns:\n",
    "        if pattern in response:\n",
    "            # Agent admitted uncertainty - this is honest, not a pass/fail\n",
    "            return True\n",
    "\n",
    "    return True  # No hallucination indicators found\n",
    "\n",
    "\n",
    "def handles_out_of_scope_correctly(input_data, output_data, expected_output):\n",
    "    \"\"\"\n",
    "    Check if the agent correctly handles out-of-scope questions.\n",
    "\n",
    "    For questions marked as should_refuse=true:\n",
    "      - Agent should refuse/redirect (not provide an answer)\n",
    "    For questions marked as should_refuse=false:\n",
    "      - Agent should provide a substantive answer\n",
    "\n",
    "    Returns True if the agent behaves correctly.\n",
    "    \"\"\"\n",
    "    response = output_data.get(\"response\", \"\").lower()\n",
    "    should_refuse = str(input_data.get(\"should_refuse\", \"false\")).lower() == \"true\"\n",
    "\n",
    "    # Indicators that the agent is refusing/redirecting\n",
    "    refusal_indicators = [\n",
    "        \"outside the scope\",\n",
    "        \"cannot help with\",\n",
    "        \"not related to\",\n",
    "        \"i can only help with\",\n",
    "        \"i'm designed to assist with\",\n",
    "        \"don't have information about\",\n",
    "        \"beyond my scope\",\n",
    "        \"not something i can\",\n",
    "        \"i apologize, but\",\n",
    "        \"i'm unable to\",\n",
    "        \"not within my\",\n",
    "        \"cannot provide\",\n",
    "        \"unable to assist\",\n",
    "        \"can't assist with\",\n",
    "        \"not permitted\",\n",
    "    ]\n",
    "\n",
    "    # Check if response contains refusal indicators\n",
    "    is_refusing = any(indicator in response for indicator in refusal_indicators)\n",
    "\n",
    "    if should_refuse:\n",
    "        # For out-of-scope/adversarial questions, agent SHOULD refuse\n",
    "        return is_refusing\n",
    "    else:\n",
    "        # For valid policy questions, agent should NOT refuse (should answer)\n",
    "        # But we also accept if it gives a substantive answer without refusing\n",
    "        # Check if response is substantive (>50 chars and doesn't primarily refuse)\n",
    "        has_substantive_content = len(response) > 50 and not is_refusing\n",
    "        return has_substantive_content or not is_refusing\n",
    "\n",
    "\n",
    "print(\"âœ… Evaluators defined (including out-of-scope handler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515a28e",
   "metadata": {},
   "source": [
    "## Test Single Sample\n",
    "\n",
    "Before running the full experiment, let's test with a single sample to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11e0360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the minimum password length requirement at wwktm?\n",
      "Expected: Passwords must be at least 12 characters\n",
      "Category: password\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed to send, dropping 1 traces to intake at http://localhost:8126/v0.5/traces: client error (Connect)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: According to wwktm's **Password & Authentication Policy**, the minimum password length requirement is **12 characters**. \n",
      "\n",
      "The policy also recommends using passphrases, avoiding common words and reused credentials, and encourages the use of password managers for employees.\n",
      "\n",
      "--------------------------------------------------\n",
      "Contains key info: True\n",
      "Response not empty: True\n",
      "No hallucination indicators: True\n",
      "Handles out-of-scope correctly: True\n"
     ]
    }
   ],
   "source": [
    "# Test with a single sample\n",
    "test_input = dataset[0][\"input_data\"]\n",
    "test_expected = dataset[0][\"expected_output\"]\n",
    "\n",
    "print(f\"Question: {test_input['question']}\")\n",
    "print(f\"Expected: {test_expected['expected_answer']}\")\n",
    "print(f\"Category: {test_input['category']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Run task\n",
    "test_config = {\"model_id\": MODEL_CLAUDE_HAIKU_4_5}\n",
    "test_output = kb_agent_task(test_input, test_config)\n",
    "\n",
    "print(f\"Response: {test_output['response'][:500]}...\" if len(test_output['response']) > 500 else f\"Response: {test_output['response']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test evaluators\n",
    "print(f\"Contains key info: {contains_key_info(test_input, test_output, test_expected)}\")\n",
    "print(f\"Response not empty: {response_not_empty(test_input, test_output, test_expected)}\")\n",
    "print(f\"No hallucination indicators: {no_hallucination_indicators(test_input, test_output, test_expected)}\")\n",
    "print(f\"Handles out-of-scope correctly: {handles_out_of_scope_correctly(test_input, test_output, test_expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bfdef",
   "metadata": {},
   "source": [
    "## Experiment 1: Claude Haiku 4.5\n",
    "\n",
    "Run the experiment with Claude Haiku 4.5 (faster, more cost-effective model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad77ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with Claude Haiku 4.5...\n",
      "\n",
      "Haiku experiment complete!\n",
      "View results: https://app.datadoghq.com/llm/experiments/e4567a9d-3d30-438d-90f5-f86779b81917\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for Claude Haiku 4.5\n",
    "experiment_haiku = LLMObs.experiment(\n",
    "    name=\"kb-agent-claude-haiku-4-5-v2\",\n",
    "    dataset=dataset,\n",
    "    task=kb_agent_task,\n",
    "    evaluators=[\n",
    "        contains_key_info,\n",
    "        response_not_empty,\n",
    "        no_hallucination_indicators,\n",
    "        handles_out_of_scope_correctly\n",
    "    ],\n",
    "    config={\n",
    "        \"model_id\": MODEL_CLAUDE_HAIKU_4_5,\n",
    "        \"model_name\": \"Claude Haiku 4.5\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Starting experiment with Claude Haiku 4.5...\")\n",
    "\n",
    "# Run the Haiku experiment\n",
    "results_haiku = experiment_haiku.run(jobs=5)  # Use 5 parallel jobs\n",
    "\n",
    "print(f\"\\nHaiku experiment complete!\")\n",
    "print(f\"View results: {experiment_haiku.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafb649",
   "metadata": {},
   "source": [
    "## Experiment 2: Amazon Nova Pro \n",
    "\n",
    "Run the experiment with Amazon Nova Pro (even cheaper than Claude Haiku 4.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972e819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with Amazon Nova Pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed to send, dropping 1 traces to intake at http://localhost:8126/v0.5/traces: client error (Connect) [19 skipped]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Nova Pro experiment complete!\n",
      "View results: https://app.datadoghq.com/llm/experiments/997ad512-0e47-4310-9a73-8953805ca0fe\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for Amazon Nova Pro\n",
    "experiment_nova_pro = LLMObs.experiment(\n",
    "    name=\"kb-agent-amazon-nova-pro-v2\",\n",
    "    dataset=dataset,\n",
    "    task=kb_agent_task,\n",
    "    evaluators=[\n",
    "        contains_key_info,\n",
    "        response_not_empty,\n",
    "        no_hallucination_indicators,\n",
    "        handles_out_of_scope_correctly\n",
    "    ],\n",
    "    config={\n",
    "        \"model_id\": MODEL_AMAZON_NOVA_PRO,\n",
    "        \"model_name\": \"Amazon Nova Pro\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Starting experiment with Amazon Nova Pro...\")\n",
    "\n",
    "# Run the Nova Pro experiment\n",
    "results_nova_pro = experiment_nova_pro.run(jobs=5)  # Use 5 parallel jobs\n",
    "\n",
    "print(f\"\\nAmazon Nova Pro experiment complete!\")\n",
    "print(f\"View results: {experiment_nova_pro.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f063e9",
   "metadata": {},
   "source": [
    "## Experiment 3: Amazon Nova Micro\n",
    "\n",
    "Run the experiment with Amazon Nova Micro (even cheaper than Amazon Nova Pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c9925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with Amazon Nova Micro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed to send, dropping 2 traces to intake at http://localhost:8126/v0.5/traces: client error (Connect) [20 skipped]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon Nova Micro experiment complete!\n",
      "View results: https://app.datadoghq.com/llm/experiments/05a01ca1-4a55-46bd-9899-0bb302838552\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for Amazon Nova Micro\n",
    "experiment_nova_micro = LLMObs.experiment(\n",
    "    name=\"kb-agent-amazon-nova-micro-v2\",\n",
    "    dataset=dataset,\n",
    "    task=kb_agent_task,\n",
    "    evaluators=[\n",
    "        contains_key_info,\n",
    "        response_not_empty,\n",
    "        no_hallucination_indicators,\n",
    "        handles_out_of_scope_correctly\n",
    "    ],\n",
    "    config={\n",
    "        \"model_id\": MODEL_AMAZON_NOVA_MICRO,\n",
    "        \"model_name\": \"Amazon Nova Micro\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Starting experiment with Amazon Nova Micro...\")\n",
    "\n",
    "# Run the Nova Pro experiment\n",
    "results_nova_micro = experiment_nova_micro.run(jobs=5)  # Use 5 parallel jobs\n",
    "\n",
    "print(f\"\\nAmazon Nova Micro experiment complete!\")\n",
    "print(f\"View results: {experiment_nova_micro.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6212b31",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Compare the experiment results and view them in Datadog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc03bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results Summary\n",
      "==================================================\n",
      "\n",
      "Claude Haiku 4.5: https://app.datadoghq.com/llm/experiments/e4567a9d-3d30-438d-90f5-f86779b81917\n",
      "\n",
      "Amazon Nova Pro: https://app.datadoghq.com/llm/experiments/997ad512-0e47-4310-9a73-8953805ca0fe\n",
      "\n",
      "Amazon Nova Micro: https://app.datadoghq.com/llm/experiments/05a01ca1-4a55-46bd-9899-0bb302838552\n",
      "\n",
      "==================================================\n",
      "\n",
      "View detailed metrics, traces and comparisons in Datadog LLM Observability!\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nClaude Haiku 4.5: {experiment_haiku.url}\")\n",
    "print(f\"\\nAmazon Nova Pro: {experiment_nova_pro.url}\")\n",
    "print(f\"\\nAmazon Nova Micro: {experiment_nova_micro.url}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nView detailed metrics, traces and comparisons in Datadog LLM Observability!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalobility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
